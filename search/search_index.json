{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#demo-on-deploying-nextflow-and-slurm-array-on-nesi-mahuika-cluster","title":"Demo on deploying Nextflow and Slurm array on NeSI Mahuika cluster","text":"<p>Nextflow</p> <p> Nextflow is workflow management software which enables the writing of scalable and reproducible scientific workflows. It can integrate various software package and environment management systems from environment modules to Docker, Singularity, and Conda. It allows for existing pipelines written in common scripting languages, such as R and Python, to be seamlessly coupled together. It implements a Domain Specific Language (DSL) that simplifies the implementation and running of workflows on cloud or high-performance computing (HPC) infrastructures.</p> <p>Nextflow example </p> <p>Slurm array</p> <p>  Job arrays offer a mechanism for submitting and managing collections of similar jobs quickly and easily; job arrays with millions of tasks can be submitted in milliseconds (subject to configured size limits). All jobs must have the same initial options (e.g. size, time limit, etc.)</p> <p>Slurm Array example using BLAST</p>"},{"location":"1_nf/","title":"Nextflow","text":"For the demonstrator <ul> <li> <p>Working directory is /nesi/project/nesi99999/Dinindu/20230503-pfr-demo/nextflow/</p> </li> <li> <p>Content of /example_1 directory is    <pre><code>main.nf  Makefile  nextflow.config  nf-clean.sh\n</code></pre></p> <ul> <li>If any of the files are missing, download them from the repo</li> </ul> </li> <li> <p>Content of /example_2 directory is    <pre><code>local_config/nesi_mahuika.config  nf_launch.sh\n</code></pre></p> </li> </ul> Example 1 <ul> <li>Objective is to create the directory /output, populate it with three files <code>bar.txt  baz.txt  foo.txt</code> where the content of each file is a single string matching the filename</li> <li>Launching the workflow can be done with  <pre><code>$ module load Nextflow/22.10.3 \n\n$ nextflow run main.nf\n</code></pre></li> </ul> <p>It's interactive</p> <ul> <li>Similar to other some workflow management systems, executing <code>nextflow run main.nf</code> will launch it as an interactive process. Therefore, it has to be launched as a background process with a utility such as <code>tmux</code> , <code>screen</code> , <code>nohup</code>. </li> <li>OR....use Slurm's <code>wrap</code> function which is much better than above options \ud83d\ude0a</li> </ul> <ul> <li>Therefore, best way to launch is</li> </ul> <p>code</p> <pre><code>sbatch --wrap 'nextflow run main.nf'\n</code></pre> <ul> <li>Check the status with </li> </ul> <p>code</p> <pre><code>#OR use `squeue -j JOBID`\nsqueue --me     \n</code></pre> <p>Can use <code>nextflow tower</code></p> <p>No restrictions (firewall rules,etc) with respect to using <code>nf tower</code>.  As long as the <code>TOWER_ACCESS_TOKEN</code> is defined on current session ( Ideally add it to ~/.bashrc), tower can be called as usual via <code>-with-tower</code> flag and runtime information will be propagated to https://tower.nf/ <pre><code>sbatch --wrap 'nextflow run main.nf -with-tower'\n</code></pre></p> Example 2 <ul> <li>This is using the \"image segmentation and extraction of single cell expression data\" pipeline provided by <code>nf-core</code> https://nf-co.re/imcyto/1.0.0</li> <li>Given this is a Singularity container based workflow, it required few variables such as <code>SINGULARITY_BIND</code>. Ideal approach is to prepare a launch script as below and then submit it with <code>sbatch</code></li> </ul> <p><pre><code>#!/bin/bash -e\n\n#Can use Apptainer as well\nmodule purge\nmodule load Singularity       \nmodule load Nextflow/22.10.3\n\n\n#Singularity and Nextflow variables\nexport SINGULARITY_BIND=\"/nesi/project,nesi/nobackup,/opt/nesi\"\nexport SINGULARITY_TMPDIR=/nesi/nobackup/nesi99999/Dinindu/cache\nexport SINGULARITY_CACHEDIR=$SINGULARITY_TMPDIR\nsetfacl -b \"$SINGULARITY_TMPDIR\"\nsetfacl -b \"/nesi/project/nesi99999/Dinindu/20230503-pfr-demo/nextflow/example_2\"\n\nexport NXF_EXECUTOR=slurm\nexport NXF_SINGULARITY_CACHEDIR=$SINGULARITY_CACHEDIR\n\nsrun nextflow run nf-core/imcyto -profile test,singularity \\\n-c local_config/nesi_mahuika.config  -resume -with-tower\n</code></pre> - Submit the script with <code>sbatch nf_launch.sh</code></p>"},{"location":"2_blast/","title":"Slurm array: Run multiple BLAST queries in parallel with a single submission script","text":"<p>BLAST finds regions of similarity between biological sequences. The program compares nucleotide or protein sequences to sequence databases and calculates the statistical significance</p> For the demonstrator <ul> <li>Working directory /nesi/project/nesi99999/Dinindu/20230503-pfr-demo/blast</li> <li>Sequences were delivered in a single file /nesi/project/nesi99999/Dinindu/20230503-pfr-demo/blast/parent_file/demo-hic.fa</li> <li>Sequence file was split to 150 separate queries by sequence with <code>faSplit</code> and stored in input-queries <pre><code>$ pwd\n/nesi/project/nesi99999/Dinindu/20230503-pfr-demo/blast\n\n$ faSplit sequence parent_file/demo-hic.fa 150 split-queries/demo-hic\n\n$ ls split-queries/\n\ndemo-hic000.fa  demo-hic019.fa  demo-hic038.fa  demo-hic057.fa  demo-hic076.fa  demo-hic095.fa  demo-hic114.fa  demo-hic133.fa\ndemo-hic001.fa  demo..............................\n</code></pre></li> <li>Slurm Submission script is /nesi/project/nesi99999/Dinindu/20230503-pfr-demo/blast/scripts/demo-array.slurm</li> <li>Query outputs will be saved to /blast-out and the Slurm StdOut to /slurm-logs</li> </ul> <p>Slurm array script</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account        nesi99999\n#SBATCH --job-name       blast_fastaSplit\n#SBATCH --cpus-per-task  1\n#SBATCH --mem            2G\n#SBATCH --time           24:00:00\n#SBATCH --array          0-149\n#SBATCH --output         /nesi/project/nesi99999/Dinindu/20230503-pfr-demo/blast/slurm-logs/%A_%a.out\n\ndate;hostname;pwd\n\nmodule load BLASTDB/2024-01\nmodule load BLAST/2.13.0-GCC-11.3.0\n\nexport INPUT_DIR=/nesi/project/nesi99999/Dinindu/20230503-pfr-demo/blast/input-queries\nexport OUTPUT_DIR=/nesi/project/nesi99999/Dinindu/20230503-pfr-demo/blast/blast-out\n\n\nRUN_ID=$(( $SLURM_ARRAY_TASK_ID + 1 ))\n\nQUERY_FILE=$( ls ${INPUT_DIR} | sed -n ${RUN_ID}p )\nQUERY_NAME=\"${QUERY_FILE%.*}\"\n\nQUERY=\"${INPUT_DIR}/${QUERY_FILE}\"\nOUTPUT=\"${OUTPUT_DIR}/${QUERY_NAME}.out\"\n\necho -e \"Command:\\nblastn \u2013query ${QUERY} \u2013db nt \u2013out ${OUTPUT} -outfmt 6 -max_target_seqs 1 -num_threads $SLURM_CPUS_PER_TASK\"\n\nblastn -query ${QUERY} -db nt -out ${OUTPUT} -outfmt 6 -max_target_seqs 1 -num_threads $SLURM_CPUS_PER_TASK \n\ndate\n</code></pre> <p>submit</p> <ul> <li>submit the script with <code>sbatch scripts/demo-array.slurm</code> <ul> <li>If needed, use array throttling (eeping only a certain number of tasks RUNNING at a time). Let's say we want to run only 20 queries at a time (out of 149), then adding <code>#SBATCH --array 0-149%20</code> to the submission script or call during submission to <code>sbatch</code> command with <code>sbatch --array 0-149%20 scripts/demo-array.slurm</code></li> </ul> </li> <li>Review the status of submission with <code>squeue -j jobid</code></li> </ul>"},{"location":"3_blast/","title":"Multiple threaded BLAST  and local database copy","text":"<p>How to find the size of a BLAST database associated with <code>BLASTDB</code> - using <code>nr</code> as an example</p> <p>Run module show for the BLASTDB module and look for the path starting with /opt/nesi.. under setenv .i.e. </p> <p><pre><code>$ module show BLASTDB/2024-01 \n\n   /opt/nesi/CS400_centos7_bdw/modules/all/BLASTDB/2024-01.lua:\n\nwhatis(\"Description: BLAST databases downloaded from NCBI.\")\nsetenv(\"BLASTDB\",\"/opt/nesi/db/blast/2024-01\")\n</code></pre> In this instance, it is <code>/opt/nesi/db/blast/2024-01</code></p> <p>Now run the following find command pointing to that path and use <code>nr</code> prefix for -name <code>nr.*</code> .  ( You can use the same command for other databases by replacing nr with the required prefix such as nt, etc</p> <pre><code>$ find  /opt/nesi/db/blast/2024-01/ -type f -name 'nr.*' -exec du -ch {} + | grep total$\n485G    total\n</code></pre>"},{"location":"3_blast/#option-1-local-copy-on-memory","title":"Option 1. local copy on Memory","text":"<p>For jobs which need more than 24 CPU-hours, eg: those that use large databases (&gt; 10 GB) or large amounts of query sequence (&gt; 1 GB), or slow BLAST searches such as classic blastn (blastn -task blastn).</p> <p>This script copies the BLAST database into the per-job temporary directory <code>$TMPDIR</code> before starting the search. Since compute nodes do not have local disks, this database copy is in memory, and so must be allowed for in the memory requested by the job</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      BLAST\n#SBATCH --time          02:30:00\n#SBATCH --mem           120G  # 30 GB plus the database\n#SBATCH --ntasks        1\n#SBATCH --cpus-per-task 36    # half a node\n\nmodule load BLAST/2.13.0-GCC-11.3.0\nmodule load BLASTDB/2024-01\n\n# This script takes one argument, the FASTA file of query sequences.\nQUERIES=$1\nFORMAT=\"6 qseqid qstart qend qseq sseqid sgi sacc sstart send staxids sscinames stitle length evalue bitscore\"\nBLASTOPTS=\"-task blastn\"\nBLASTAPP=blastn\nDB=nt\n#BLASTAPP=blastx\n#DB=nr\n\n# Keep the database in RAM\ncp $BLASTDB/{$DB,taxdb}.* $TMPDIR/ \nexport BLASTDB=$TMPDIR\n\n$BLASTAPP $BLASTOPTS -db $DB -query $QUERIES -outfmt \"$FORMAT\" \\\n    -out $QUERIES.$DB.$BLASTAPP -num_threads $SLURM_CPUS_PER_TASK\n</code></pre>"},{"location":"3_blast/#option-2-local-copy-on-milan-partition-local-ssd","title":"Option 2. Local copy on milan partition local SSD","text":"<ul> <li>NeSI Mahuika cluster <code>milan</code> partition compute nodes comes with local NVMe SSDs which can host up to ~1.5TB of data during runtime</li> <li>Base Slurm script will be very similar to above with following two Slurm variables</li> </ul> <pre><code>#SBATCH --partition milan\n#SBATCH --gres=ssd\n</code></pre> <ul> <li>Also we can lower the memory to Blast operational requirements </li> </ul> <pre><code>#!/bin/bash -e\n\n#SBATCH --job-name      BLAST\n#SBATCH --time          02:30:00\n#SBATCH --mem           120G  # 30 GB plus the database\n#SBATCH --ntasks        1\n#SBATCH --cpus-per-task 64    # half a node\n#SBATCH --mem           60G\n#SBATCH --partition     milan\n#SBATCH --gres          ssd\n\nmodule load BLAST/2.13.0-GCC-11.3.0\nmodule load BLASTDB/2024-01\n\n# This script takes one argument, the FASTA file of query sequences.\nQUERIES=$1\nFORMAT=\"6 qseqid qstart qend qseq sseqid sgi sacc sstart send staxids sscinames stitle length evalue bitscore\"\nBLASTOPTS=\"-task blastn\"\nBLASTAPP=blastn\nDB=nt\n#BLASTAPP=blastx\n#DB=nr\n\n# Keep the database in RAM\ncp $BLASTDB/{$DB,taxdb}.* $TMPDIR/ \nexport BLASTDB=$TMPDIR\n\n$BLASTAPP $BLASTOPTS -db $DB -query $QUERIES -outfmt \"$FORMAT\" \\\n    -out $QUERIES.$DB.$BLASTAPP -num_threads $SLURM_CPUS_PER_TASK\n</code></pre>"}]}